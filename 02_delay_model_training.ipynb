{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45275e63-04cb-4b49-a9a4-3b6cb4416608",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b9559c-9638-4b1f-a3f8-5f8ca0343289",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Start Spark name:{spark._sc.appName}, version:{spark.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a1e7c-e495-40d4-8871-fbdd442b79be",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = spark.conf.get(\"spark.executorEnv.USERNAME\", \"default_value\")\n",
    "username"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d892673-ce68-4907-8d9e-545553636ecd",
   "metadata": {},
   "source": [
    "### Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ebf92-ed10-4fcb-9ceb-48758d96ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf, count, lag, col, to_timestamp, concat_ws, lit, lpad\n",
    "from pyspark.sql.functions import col, to_date, to_timestamp, year, month, dayofmonth, dayofweek, expr, when, row_number, count\n",
    "from pyspark.sql.types import BooleanType, IntegerType, DoubleType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2dc77-4db5-4279-9915-356d15a8e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# List files in the directory\n",
    "files = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration()) \\\n",
    "    .listStatus(sc._jvm.org.apache.hadoop.fs.Path('/data/geo/json'))\n",
    "for file_status in files:\n",
    "    print(file_status.getPath())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e9ecb-f013-4f92-abef-04643d0abb83",
   "metadata": {},
   "source": [
    "## Istdaten Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bbeabc-c7da-4c97-865a-94a066673d18",
   "metadata": {},
   "source": [
    "### 1) Select Relevant Columns and Rename for Convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3316c4c-4dee-43ab-8a94-f9917cb5f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "istdaten_df = spark.read.orc('/data/sbb/orc/istdaten/year=2023')\n",
    "\n",
    "istdaten_df = istdaten_df.select([\n",
    "      istdaten_df['betriebstag'].alias('date'),\n",
    "      istdaten_df['fahrt_bezeichner'].alias('trip_id'),\n",
    "      istdaten_df['betreiber_id'].alias('operator_id'),\n",
    "      istdaten_df['bpuic'].alias('stop_id'),\n",
    "      istdaten_df['produkt_id'].alias('transportation'),\n",
    "      istdaten_df['faellt_aus_tf'].alias('cancelled'),\n",
    "      istdaten_df['ankunftszeit'].alias('arrival_time'),\n",
    "      istdaten_df['an_prognose'].alias('arrival_prognosis'),\n",
    "      istdaten_df['an_prognose_status'].alias('arr_prog_status'),\n",
    "      istdaten_df['abfahrtszeit'].alias('departure_time'),\n",
    "      istdaten_df['ab_prognose'].alias('departure_prognosis'),\n",
    "      istdaten_df['ab_prognose_status'].alias('dep_prog_status'),\n",
    "])\n",
    "\n",
    "stops_df = spark.read.csv('/data/sbb/csv/timetables/stops', header=True)\\\n",
    "                .select(['stop_id','stop_lat', 'stop_lon'])\\\n",
    "                .drop(*['year','month','day'])\\\n",
    "                .dropDuplicates(['stop_id'])\\\n",
    "                .withColumnRenamed(\"stop_id\", \"stops_stop_id\")\\\n",
    "                .withColumn(\"stop_lat\", col(\"stop_lat\").cast(DoubleType()))\\\n",
    "                .withColumn(\"stop_lon\", col(\"stop_lon\").cast(DoubleType()))\n",
    "\n",
    "istdaten_df = istdaten_df.join(\n",
    "    stops_df,\n",
    "    istdaten_df.stop_id == stops_df.stops_stop_id,\n",
    "    \"left\"\n",
    ").drop('stops_stop_id')\n",
    "\n",
    "istdaten_df.show(5)\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03efc8cc-4646-4377-823b-42680e4eb955",
   "metadata": {},
   "source": [
    "### 2) Filtering and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234fae2-fcce-4a51-b0d6-5823c2a3ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_arr_prog_status = istdaten_df.select(\"arr_prog_status\").distinct().collect()\n",
    "distinct_arr_prog_status_list = [row.arr_prog_status for row in distinct_arr_prog_status]\n",
    "print(distinct_arr_prog_status_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86eeb2-6cce-42f2-a363-b998fb1de332",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_date = \"dd.MM.yyyy\"\n",
    "format_timetable = \"dd.MM.yyyy HH:mm\"\n",
    "format_prognosis = \"dd.MM.yyyy HH:mm:ss\"\n",
    "\n",
    "istdaten_df = istdaten_df.filter(istdaten_df.arr_prog_status=='REAL')\\\n",
    "                         .withColumn(\"date\", to_timestamp(col(\"date\"), format_date))\\\n",
    "                         .withColumn(\"arrival_time\", to_timestamp(col(\"arrival_time\"), format_timetable))\\\n",
    "                         .withColumn(\"arrival_prognosis\", to_timestamp(col(\"arrival_prognosis\"), format_prognosis))\\\n",
    "                         .withColumn(\"departure_time\", to_timestamp(col(\"departure_time\"), format_timetable))\\\n",
    "                         .withColumn(\"departure_prognosis\", to_timestamp(col(\"departure_prognosis\"), format_prognosis))\n",
    "\n",
    "istdaten_df = istdaten_df.filter((col(\"arr_prog_status\") != \"\") & (col(\"arr_prog_status\") != \"UNBEKANNT\") & (col('cancelled') == 'false'))\n",
    "\n",
    "istdaten_df.show(5)\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acae04f-ea3e-4f71-82c5-de0fa9f964da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = f\"/user/{username}/stop_matching.csv\"\n",
    "stop_in_region = spark.read.csv(hdfs_path, header=\"True\")\n",
    "stop_in_region.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8b20a-1f3c-41b8-a526-93e0c6f6a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_ids_to_filter = stop_in_region.select(\"isdaten_stop_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "istdaten_df = istdaten_df.filter(istdaten_df.stop_id.isin(stop_ids_to_filter))\n",
    "\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e5656-ba7e-433c-9994-8eec3b7f6db1",
   "metadata": {},
   "source": [
    "### 3) Istdaten Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb7ba9-920e-4a00-b384-6d28979230bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "istdaten_df = istdaten_df.withColumn(\"delay_arrival\", (col(\"arrival_prognosis\").cast(\"long\") - col(\"arrival_time\").cast(\"long\")))\\\n",
    "                         .withColumn(\"delay_departure\", (col(\"departure_prognosis\").cast(\"long\") - col(\"departure_time\").cast(\"long\")))\\\n",
    "                         .withColumn(\"delay_at_station\", (col(\"delay_departure\") - col(\"delay_arrival\")))\\\n",
    "                         .withColumn(\"year\", year(col(\"date\")))\\\n",
    "                         .withColumn(\"month\", month(col(\"date\")))\\\n",
    "                         .withColumn(\"day\", dayofmonth(col(\"date\")))\\\n",
    "                         .withColumn(\"day_of_week\", (dayofweek(col(\"date\")) + 5) % 7)\\\n",
    "                         .withColumn(\"is_weekday\", (col(\"day_of_week\") <= 4))\\\n",
    "                         .withColumn(\"is_weekend\", (col(\"day_of_week\") > 4))\\\n",
    "                         .withColumn(\"is_peak_time\",\n",
    "                                     when((col(\"is_weekday\")) & \n",
    "                                          ((col(\"arrival_time\").between(expr(\"make_timestamp(year(date), month(date), day(date), 6, 30, 0)\"), \n",
    "                                                                        expr(\"make_timestamp(year(date), month(date), day(date), 8, 30, 0)\"))) |\n",
    "                                           (col(\"arrival_time\").between(expr(\"make_timestamp(year(date), month(date), day(date), 16, 30, 0)\"), \n",
    "                                                                        expr(\"make_timestamp(year(date), month(date), day(date), 18, 30, 0)\")))), True)\n",
    "                                     .otherwise(False))\n",
    "istdaten_df.show(5)\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd210b-4c5b-493d-8634-fb208efe803f",
   "metadata": {},
   "source": [
    "### 4) Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58941c2-5b49-42f8-b4ac-7bd989f60da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haversine formula to calculate the distance between two points on the earth\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    if lat1 is None or lon1 is None or lat2 is None or lon2 is None:\n",
    "        return None\n",
    "    R = 6371  # Radius of the Earth in kilometers\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dlat / 2) * math.sin(dlat / 2) + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon / 2) * math.sin(dlon / 2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c  # Distance in kilometers\n",
    "    return distance\n",
    "\n",
    "# Register the UDF\n",
    "haversine_udf = udf(haversine, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd21b49b-09d5-41de-96a1-dda1474a7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window spec for calculating trip_stop_seq with year, month, and day\n",
    "trip_stop_seq_window = Window.partitionBy(\"trip_id\", \"year\", \"month\", \"day\").orderBy(\"arrival_time\")\n",
    "istdaten_df = istdaten_df.withColumn(\"trip_stop_seq\", row_number().over(trip_stop_seq_window))\n",
    "\n",
    "# Define the window spec for calculating trip_num_stops with year, month, and day\n",
    "trip_num_stops_window = Window.partitionBy(\"trip_id\", \"year\", \"month\", \"day\")\n",
    "istdaten_df = istdaten_df.withColumn(\"trip_num_stops\", count(\"stop_id\").over(trip_num_stops_window))\n",
    "\n",
    "# Add columns for the previous stop_id and departure_time\n",
    "window_spec_seq = Window.partitionBy(\"trip_id\", \"year\", \"month\", \"day\").orderBy(\"trip_stop_seq\")\n",
    "istdaten_df = istdaten_df.withColumn(\"prev_stop_id\", lag(\"stop_id\", 1).over(window_spec_seq))\\\n",
    "                         .withColumn(\"prev_departure_time\", lag(\"departure_time\", 1).over(window_spec_seq))\\\n",
    "                         .withColumn(\"prev_departure_prognosis\", lag(\"departure_prognosis\", 1).over(window_spec_seq))\\\n",
    "                         .withColumn(\"prev_stop_lat\", lag(\"stop_lat\", 1).over(window_spec_seq))\\\n",
    "                         .withColumn(\"prev_stop_lon\", lag(\"stop_lon\", 1).over(window_spec_seq))\\\n",
    "                         .withColumn(\"scheduled_traveling_time_min\", ((col(\"arrival_time\").cast(\"long\") - col(\"prev_departure_time\").cast(\"long\"))) / 60)\\\n",
    "                         .withColumn(\"actual_traveling_time_min\", ((col(\"arrival_prognosis\").cast(\"long\") - col(\"prev_departure_prognosis\").cast(\"long\"))) / 60)\\\n",
    "                         .withColumn(\"traveling_distance_km\", haversine_udf(col(\"prev_stop_lat\"), col(\"prev_stop_lon\"),col(\"stop_lat\"), col(\"stop_lon\")))\n",
    "\n",
    "# Define a window spec ordered by trip identifier\n",
    "windowSpec = Window.partitionBy(\"trip_id\").orderBy(\"departure_time\") \n",
    "\n",
    "# Create columns for previous stop departure time and prognose\n",
    "istdaten_df = istdaten_df.withColumn(\"prev_delay\", lag(\"delay_arrival\", 1).over(windowSpec))\n",
    "\n",
    "istdaten_df.show(5)\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c003e-727c-45ab-a3c6-28474f4ce1f8",
   "metadata": {},
   "source": [
    "### Weather Data [Optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446fbbb-392a-4d8b-aa04-029302ae8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Calculate the distance between each stop and each weather station\n",
    "wstations_df = spark.read.csv('/data/wunderground/csv/stations', header=True)\n",
    "distance_df = stops_df.crossJoin(wstations_df).withColumn(\n",
    "    \"distance\",\n",
    "    haversine_udf(col(\"stop_lat\"),col(\"stop_lon\"), col(\"lat_wgs84\").cast(DoubleType()), col(\"lon_wgs84\").cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# Using window function to find the nearest station\n",
    "windowSpec = Window.partitionBy(\"stops_stop_id\").orderBy(\"distance\")\n",
    "nearest_stations = distance_df.withColumn(\"rank\", row_number().over(windowSpec)) \\\n",
    "                              .filter(col(\"rank\") == 1) \\\n",
    "                              .drop(\"rank\")\\\n",
    "                              .select(['stops_stop_id', 'site', 'lat_wgs84', 'lon_wgs84', 'distance'])\n",
    "\n",
    "# Show the results\n",
    "nearest_stations.show(5)\n",
    "nearest_stations.printSchema()\n",
    "nearest_stations.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7865d45-3306-44ee-923a-207886df3a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, to_timestamp, round, from_unixtime, hour\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "weather_data_df = spark.read.orc('/data/share/weather_win3h_avg_df.orc')\n",
    "\n",
    "# Merge weather data with station coordinates\n",
    "weather_stations_df = weather_data_df.join(\n",
    "    nearest_stations,\n",
    "    weather_data_df.site == nearest_stations.site,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Select necessary columns including coordinates in WGS84 which will help in geographical matching\n",
    "weather_stations_df = weather_stations_df.select(\n",
    "    \"valid_time_gmt\", \"temp\", \"avg_temp\", \"lat_wgs84\", \"lon_wgs84\", \"stops_stop_id\"\n",
    ").withColumn(\"weather_hour\", (col(\"valid_time_gmt\") / 3600).cast(\"integer\") * 3600)\n",
    "\n",
    "istdaten_df = istdaten_df.withColumn(\"arrival_unix\", unix_timestamp(to_timestamp(col(\"arrival_time\"))))\\\n",
    "                         .withColumn(\"arrival_unix_hour\", (col(\"arrival_unix\") / 3600).cast(\"integer\") * 3600)\n",
    "\n",
    "istdaten_df = istdaten_df.join(\n",
    "    weather_stations_df,\n",
    "    (istdaten_df.stop_id == weather_stations_df.stops_stop_id) &\n",
    "    (istdaten_df.arrival_unix_hour == weather_stations_df.weather_hour),\n",
    "    \"left_outer\"  # left_outer to keep transport data regardless of match\n",
    ")\n",
    "\n",
    "istdaten_df.show(5)\n",
    "istdaten_df.printSchema()\n",
    "istdaten_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df76e7-d17a-449b-85d4-def4ce2fa51f",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f7fc3d-bb97-4935-88f1-3c317347207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "\n",
    "cols_to_keep = ['year' ,'month' ,'day' ,'day_of_week' ,'is_weekday' ,'is_weekend','is_peak_time', \n",
    " 'trip_id', 'stop_id','prev_stop_id','transportation', 'arrival_time', 'stop_lat','stop_lon', 'prev_stop_lat', 'prev_stop_lon','delay_arrival', 'delay_at_station',\n",
    "'trip_stop_seq','trip_num_stops','scheduled_traveling_time_min','actual_traveling_time_min','traveling_distance_km','prev_delay','temp','avg_temp','lat_wgs84','lon_wgs84']\n",
    "\n",
    "features = istdaten_df.select(cols_to_keep)\\\n",
    "                      .withColumn(\"arrival_hour\", hour(col(\"arrival_time\")))\\\n",
    "\n",
    "features.show(5)\n",
    "features.printSchema()\n",
    "features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c70c10-da33-4927-940a-360231049683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Compute mean and std deviation for relevant columns per stop_id\n",
    "statistics_df = features.groupBy('stop_id').agg(\n",
    "    F.mean('delay_arrival').alias('station_mean_delay_arrival'),\n",
    "    F.stddev('delay_arrival').alias('station_stddev_delay_arrival'),\n",
    "    F.mean('scheduled_traveling_time_min').alias('station_mean_scheduled_traveling_time_min'),\n",
    "    F.stddev('scheduled_traveling_time_min').alias('station_stddev_scheduled_traveling_time_min'),\n",
    "    F.mean('actual_traveling_time_min').alias('station_mean_actual_traveling_time_min'),\n",
    "    F.stddev('actual_traveling_time_min').alias('station_stddev_actual_traveling_time_min'),\n",
    "    F.mean('traveling_distance_km').alias('station_mean_traveling_distance_km'),\n",
    "    F.stddev('traveling_distance_km').alias('station_stddev_traveling_distance_km')\n",
    ").withColumn(\n",
    "    'mean_traveling_time_delta', \n",
    "    F.col('station_mean_actual_traveling_time_min') - F.col('station_mean_scheduled_traveling_time_min')\n",
    ")\n",
    "\n",
    "# Join the statistics for current stop\n",
    "features_with_stats = features.join(statistics_df, features.stop_id == statistics_df.stop_id, how='left').drop(statistics_df.stop_id)\n",
    "\n",
    "# Prepare statistics for previous stop with renamed columns\n",
    "prev_statistics_df = statistics_df.select(*[F.col(x).alias('prev_' + x) for x in statistics_df.columns])\n",
    "\n",
    "# Join the statistics for previous stop\n",
    "features_with_stats = features_with_stats.join(prev_statistics_df, features_with_stats.prev_stop_id == prev_statistics_df.prev_stop_id, how='left').drop(prev_statistics_df.prev_stop_id)\n",
    "\n",
    "# Show the result\n",
    "features_with_stats.printSchema()\n",
    "features_with_stats.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d6dd8-aac7-4699-9c34-aef6473b8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Aggregate on [stop_id, prev_stop_id]\n",
    "edge_stats_df = features.groupBy('stop_id', 'prev_stop_id').agg(\n",
    "    F.mean('scheduled_traveling_time_min').alias('edge_mean_scheduled_traveling_time_min'),\n",
    "    F.stddev('scheduled_traveling_time_min').alias('edge_stddev_scheduled_traveling_time_min'),\n",
    "    F.mean('actual_traveling_time_min').alias('edge_mean_actual_traveling_time_min'),\n",
    "    F.stddev('actual_traveling_time_min').alias('edge_stddev_actual_traveling_time_min')\n",
    ")\n",
    "\n",
    "# Step 2: Join the computed statistics back to the original dataframe\n",
    "features_with_edge_stats = features_with_stats.join(\n",
    "    edge_stats_df, \n",
    "    (features_with_stats.stop_id == edge_stats_df.stop_id) & (features_with_stats.prev_stop_id == edge_stats_df.prev_stop_id), \n",
    "    how='left'\n",
    ").drop(edge_stats_df.stop_id).drop(edge_stats_df.prev_stop_id)\n",
    "\n",
    "# Optional: Show the results and schema to verify correctness\n",
    "features_with_edge_stats.show(5)\n",
    "features_with_stats.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca128b5b-7459-4666-a39b-3c86f2c2d6f6",
   "metadata": {},
   "source": [
    "### Null value processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040d327b-d60d-4ec2-b855-9d509c7c7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NaNs for categorical and numerical columns\n",
    "categorical_cols = ['transportation']\n",
    "features_with_edge_stats = features_with_edge_stats.na.fill(\"unknown\", subset=categorical_cols)\n",
    "features_with_edge_stats = features_with_edge_stats.na.replace(\"\", \"unknown\", subset=categorical_cols)\n",
    "\n",
    "boolean_cols = [\"is_weekday\", \"is_peak_time\"]\n",
    "for col_name in boolean_cols:\n",
    "    features_with_edge_stats = features_with_edge_stats.withColumn(col_name, when(col(col_name), 1).otherwise(0))\n",
    "\n",
    "numerical_cols = ['delay_arrival','arrival_hour','day','month','year','day_of_week','stop_lat','stop_lon','prev_stop_lat','prev_stop_lon','trip_stop_seq','trip_num_stops',\n",
    " 'scheduled_traveling_time_min','traveling_distance_km','edge_mean_scheduled_traveling_time_min','edge_stddev_scheduled_traveling_time_min','edge_mean_actual_traveling_time_min',\n",
    " 'edge_stddev_actual_traveling_time_min','station_mean_delay_arrival','station_stddev_delay_arrival','prev_station_mean_delay_arrival','prev_station_stddev_delay_arrival',\n",
    " 'actual_traveling_time_min','traveling_distance_km','delay_arrival','temp','avg_temp']\n",
    "\n",
    "features_with_edge_stats = features_with_edge_stats.na.fill(0, subset=numerical_cols)\n",
    "features_with_edge_stats = features_with_edge_stats.na.fill(0, subset=['delay_arrival'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e48183a-b16a-462a-8b77-00f4bc73c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "hdfs_path = f\"/user/{username}/features_with_edge_stats.parquet\"\n",
    "features_with_edge_stats.write.mode(\"overwrite\").parquet(hdfs_path)\n",
    "\n",
    "print(f\"DataFrame saved to {hdfs_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c97f4e9-92c3-4475-837a-c13f85ded696",
   "metadata": {},
   "source": [
    "### Fit Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d89772-9a06-43de-b879-408154f0ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the stages in the pipeline\n",
    "stages = []\n",
    "for column in categorical_cols:\n",
    "    string_indexer = StringIndexer(inputCol=column, outputCol=column + \"_idx\", handleInvalid=\"keep\")\n",
    "    encoder = OneHotEncoder(inputCols=[string_indexer.getOutputCol()], outputCols=[column + \"classVec\"])\n",
    "    stages.append(string_indexer)\n",
    "    stages.append(encoder)\n",
    "\n",
    "# Append the VectorAssembler and the corrected model to the pipeline stages\n",
    "all_features = [col + \"classVec\" for col in categorical_cols] + numerical_cols\n",
    "vectorAssembler = VectorAssembler(inputCols=all_features, outputCol=\"assembled_features\", handleInvalid=\"skip\")\n",
    "stages.append(vectorAssembler)\n",
    "\n",
    "# Apply StandardScaler to scale the features \n",
    "scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"features\") \n",
    "stages.append(scaler)\n",
    "\n",
    "# Initialise a Random forest-model for binary classification\n",
    "model = RandomForestRegressor(featuresCol='features', labelCol='delay_arrival', numTrees=100) \n",
    "stages.append(model)\n",
    "\n",
    "# Initialise pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Proceed to train the model\n",
    "trainData, testData = features_with_edge_stats.randomSplit([0.8, 0.2], seed=42)\n",
    "pipelineModel = pipeline.fit(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42acb424-c6ba-41be-a2ab-086592261d39",
   "metadata": {},
   "source": [
    "### Predict and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca5c05-e143-4d9e-a8a7-72bfb7ad78fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = pipelineModel.transform(testData)\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"delay_arrival\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"delay_arrival\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"delay_arrival\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# Compute the metrics\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R²): {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2423d3a-5107-46fa-9de9-53ed58ef7679",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"delay_arrival\", \"prediction\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da00c5-6c03-428b-84a3-100d53f74f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "hdfs_path = f\"/user/{username}/models\"\n",
    "pipelineModel.write().overwrite().save(hdfs_path)\n",
    "print(f\"Pipeline model saved to {hdfs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea814bb3-4f30-43f8-be66-f6d395309af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1880bff7-3b27-4b94-bb39-c51c44414f47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
