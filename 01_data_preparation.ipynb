{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "744a58a9-f3f3-4f32-ba3b-5ce15c468434",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69719ad1-71df-4ba1-8602-159cc938983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO TEACHING STAFF: \n",
    "## change the object_id field here\n",
    "object_id = 1 # 1 = Lausanne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f402757f-4d8c-4ea7-9365-9b228906bd3c",
   "metadata": {},
   "source": [
    "The following code will get (and save locally) all the data you will need to run the journey finder in the selected region.\n",
    "\n",
    "Note that we have already provided you with the Lausanne data (objectid = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb732f-1abb-4671-a063-e2ad19b5fa08",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c217fb-8c6d-40ea-9b8b-f1477085e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from pyhive import hive\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "default_db = 'com490'\n",
    "hive_server = os.environ.get('HIVE_SERVER','iccluster080.iccluster.epfl.ch:10000')\n",
    "hadoop_fs = os.environ.get('HADOOP_DEFAULT_FS','hdfs://iccluster067.iccluster.epfl.ch:8020')\n",
    "username  = os.environ.get('USER', 'anonym')\n",
    "(hive_host, hive_port) = hive_server.split(':')\n",
    "\n",
    "conn = hive.connect(\n",
    "    host=hive_host,\n",
    "    port=hive_port,\n",
    "    username=username\n",
    ")\n",
    "\n",
    "# create cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(f\"hadoop hdfs URL is {hadoop_fs}\")\n",
    "print(f\"your username is {username}\")\n",
    "print(f\"you are connected to {hive_host}:{hive_port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54412c4-5c7d-45e1-9982-3f0fabea0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the tables in default_db\n",
    "query = f\"\"\"\n",
    "    SHOW TABLES IN {default_db}\n",
    "\"\"\"\n",
    "\n",
    "cur.execute(query)\n",
    "default_db_tables = cur.fetchall()\n",
    "default_db_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577bd7b3-b0d4-40ed-8ed2-4fb812f1ffbb",
   "metadata": {},
   "source": [
    "## 2. Enable support for ESRI UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d30863-7369-45db-925d-b084505bdeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query JAR files currently available in the session\n",
    "cur.execute(\"LIST JARS\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04d565-b862-4187-a1d0-a0206562b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents of the directory '/data/jars' in HDFS -> JAR files to be added\n",
    "cur.execute(\"DFS -ls /data/jars\")\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Construct full HDFS paths for each JAR file\n",
    "jar_paths = [os.path.join(hadoop_fs, x[0].split()[-1][1:]) for x in results[1:]]\n",
    "jar_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021df97-9e19-4053-a447-7964e8388179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct and execute query to add all the JAR files\n",
    "add_jars_query = \"ADD JARS\\n\\t\" + \"\\n\\t\".join(jar_paths)\n",
    "cur.execute(add_jars_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e9f92-5298-4ac5-be5c-34741c932098",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Check that JAR files were added\n",
    "cur.execute(\"LIST JARS\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da1d939-70c9-44a8-a3b1-9f5d29cc1936",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"CREATE TEMPORARY FUNCTION ST_Point AS 'com.esri.hadoop.hive.ST_Point'\")\n",
    "cur.execute(\"CREATE TEMPORARY FUNCTION ST_Distance AS 'com.esri.hadoop.hive.ST_Distance'\")\n",
    "cur.execute(\"CREATE TEMPORARY FUNCTION ST_SetSRID AS 'com.esri.hadoop.hive.ST_SetSRID'\")\n",
    "cur.execute(\"CREATE TEMPORARY FUNCTION ST_GeodesicLengthWGS84 AS 'com.esri.hadoop.hive.ST_GeodesicLengthWGS84'\")\n",
    "cur.execute(\"CREATE TEMPORARY FUNCTION ST_LineString AS 'com.esri.hadoop.hive.ST_LineString'\")\n",
    "cur.execute(\"CREATE TEMPORARY FUNCTION ST_AsBinary AS 'com.esri.hadoop.hive.ST_AsBinary'\")\n",
    "cur.execute(\"CREATE TEMPORARY FUNCTION ST_PointFromWKB AS 'com.esri.hadoop.hive.ST_PointFromWKB'\")\n",
    "cur.execute(\"CREATE TEMPORARY FUNCTION ST_GeomFromWKB AS 'com.esri.hadoop.hive.ST_GeomFromWKB'\")\n",
    "cur.execute(\"CREATE TEMPORARY FUNCTION ST_Contains AS 'com.esri.hadoop.hive.ST_Contains'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b09dac1-2e3a-4f88-b000-8988eae809b6",
   "metadata": {},
   "source": [
    "## 3. Region Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dfafbf-b079-4d38-9722-205a9b78382d",
   "metadata": {},
   "source": [
    "The following query selects all the stops in the specified region.\n",
    "We decided to save them both on a file, to use it in the ui, and also in a table, to query it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab2d5e-de6e-4306-9a57-bc162b55b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: OBJECT_ID SHOULD BE MODIFIED AT TOP OF FILE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1672e9c-a7f9-4126-a6c3-8f55ae6c5040",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {username}.sbb_stops_in_region(\n",
    "        stop_id        string,\n",
    "        stop_name      string,\n",
    "        stop_lat       double,\n",
    "        stop_lon       double\n",
    "    )\n",
    "    STORED AS ORC\n",
    "\"\"\"\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2bd416-2cce-4260-9878-5345a0ddfe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cur.execute(f\"\"\"\n",
    "    INSERT OVERWRITE TABLE {username}.sbb_stops_in_region\n",
    "    SELECT a.stop_id, a.stop_name, a.stop_lat, a.stop_lon\n",
    "    FROM {default_db}.sbb_orc_stops a JOIN {default_db}.geo_shapes b\n",
    "    WHERE b.objectid={object_id}\n",
    "    AND ST_Contains(b.geometry, ST_Point(stop_lon,stop_lat))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b854850-dfc5-472f-a8e9-82b31133ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = f\"\"\"\n",
    "    SELECT \n",
    "        a.stop_id as stop_id,\n",
    "        a.stop_name as stop_name,\n",
    "        a.stop_lat as stop_lat,\n",
    "        a.stop_lon as stop_lon\n",
    "    FROM {default_db}.sbb_orc_stops a JOIN {default_db}.geo_shapes b\n",
    "    WHERE b.objectid={object_id}\n",
    "    AND ST_Contains(b.geometry, ST_Point(stop_lon,stop_lat))\n",
    "\"\"\"\n",
    "\n",
    "region_stops = pd.read_sql(query, conn)\n",
    "region_stops.head()\n",
    "\n",
    "data_path = \"data/\"\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "region_stops.to_csv(data_path + \"stops.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f8226-e62d-41ee-8644-fc76f24c9d5e",
   "metadata": {},
   "source": [
    "## 4. Compute Footpaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630581d-d5cc-4d54-bcef-b1c974248bb8",
   "metadata": {},
   "source": [
    "The following query selects all the pairs of stops with a distance of at most 500m and then compute the walking distances between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ac706-903a-4e64-97ee-1beac5118a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT a.stop_id as stop_id_a, b.stop_id as stop_id_b, ST_GeodesicLengthWGS84(ST_SetSRID(ST_LineString(a.stop_lon,a.stop_lat,b.stop_lon,b.stop_lat), 4326)) as distance\n",
    "    FROM {username}.sbb_stops_in_region a\n",
    "    JOIN {username}.sbb_stops_in_region b\n",
    "    WHERE a.stop_id != b.stop_id\n",
    "    AND ST_GeodesicLengthWGS84(\n",
    "            ST_SetSRID(ST_LineString(a.stop_lon,a.stop_lat,b.stop_lon,b.stop_lat), 4326)\n",
    "        ) < 500\n",
    "\"\"\"\n",
    "\n",
    "df_near_stops = pd.read_sql(query, conn)\n",
    "df_near_stops = df_near_stops.drop_duplicates()\n",
    "df_near_stops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c010f-1d16-4bcb-bb25-3761115ca11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add duration\n",
    "walk_speed = 50/60 # (50m/1min) m/s \n",
    "df_near_stops['duration'] = df_near_stops['distance'] / walk_speed\n",
    "\n",
    "df_reversed = df_near_stops.rename(columns={'stop_id_a': 'stop_id_a_noconflict', 'stop_id_b': 'stop_id_b_noconflict'})\n",
    "df_reversed = df_reversed.rename(columns={'stop_id_a_noconflict': 'stop_id_b', 'stop_id_b_noconflict': 'stop_id_a'})\n",
    "df_near_stops = pd.concat([df_near_stops, df_reversed], ignore_index=True)\n",
    "\n",
    "df_near_stops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb23849d-27cd-4ae1-85c6-b74b93213886",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_path = \"data/\"\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "df_near_stops.to_csv(data_path + \"footpaths.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff429e30-ea06-410d-b039-533ab408701c",
   "metadata": {},
   "source": [
    "## 4. Timetable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3630dd-0837-406e-bedd-fe5a374c9dde",
   "metadata": {},
   "source": [
    "This query retrieves all trips and their corresponding details for each stop within the specified region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a158d9-847e-4bd5-8b92-09e33aae0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT\n",
    "        times.trip_id AS trip_id,\n",
    "        times.stop_id AS stop_id,\n",
    "        times.arrival_time AS arrival_time,\n",
    "        times.departure_time AS departure_time,\n",
    "        trips.monday as monday,\n",
    "        trips.tuesday as tuesday,\n",
    "        trips.wednesday as wednesday,\n",
    "        trips.thursday as Thursday,\n",
    "        trips.friday as friday,\n",
    "        trips.saturday as saturday,\n",
    "        trips.sunday as sunday,\n",
    "        trips.start_date as start_date,\n",
    "        trips.end_date as end_date\n",
    "    FROM \n",
    "        {default_db}.sbb_orc_stop_times times\n",
    "    JOIN (\n",
    "        SELECT *\n",
    "        FROM {default_db}.sbb_orc_trips trips\n",
    "        JOIN {default_db}.sbb_orc_calendar calendar\n",
    "        ON trips.service_id = calendar.service_id\n",
    "    ) trips\n",
    "    ON times.trip_id = trips.trip_id\n",
    "    JOIN \n",
    "        {username}.sbb_stops_in_region region\n",
    "    ON times.stop_id = region.stop_id\n",
    "\"\"\"\n",
    "# Print statements for validation\n",
    "df_stops = pd.read_sql(query, conn)\n",
    "print('stop_times found:', df_stops.shape[0])\n",
    "print('random samples:')\n",
    "df_stops.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a0958-c1b8-48c7-beac-11d829824518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_schedule_dataframe(schedule):\n",
    "    \"\"\"\n",
    "    Constructs a dataframe of stop-to-stop connections from a transit schedule within a 2-hour window from the start time.\n",
    "\n",
    "    Parameters:\n",
    "    - schedule: DataFrame with columns ['departure_time', 'arrival_time', 'trip_id', 'stop_id']\n",
    "    - start_time: start time of the trip as a string in HH:MM:SS format\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame with columns ['Stop_a', 'Stop_b', 'Trip_id', 'Departure_time', 'Arrival_time']\n",
    "    \"\"\"\n",
    "\n",
    "    schedule['arr_time'] = pd.to_timedelta(schedule['arrival_time']).dt.total_seconds()\n",
    "    schedule['dep_time'] = pd.to_timedelta(schedule['departure_time']).dt.total_seconds()\n",
    "    schedule.sort_values(by=['trip_id', 'arr_time'])\n",
    "\n",
    "    timetable_obs = []\n",
    "    schedule_n = len(schedule)\n",
    "    for i in range(schedule_n-1):\n",
    "        if i % 50000 == 0:\n",
    "            print('fraction done: ', i/schedule_n)\n",
    "        if schedule.iloc[i]['trip_id'] == schedule.iloc[i+1]['trip_id']:\n",
    "            timetable_obs.append([schedule.iloc[i]['stop_id'], schedule.iloc[i+1]['stop_id'], schedule.iloc[i]['dep_time'], schedule.iloc[i+1]['arr_time'], schedule.iloc[i]['trip_id'], schedule.iloc[i]['monday'], schedule.iloc[i]['tuesday'], schedule.iloc[i]['wednesday'], schedule.iloc[i]['thursday'], schedule.iloc[i]['friday'], schedule.iloc[i]['saturday'], schedule.iloc[i]['sunday']])\n",
    "    timetable = pd.DataFrame(timetable_obs, columns=['dep_stop', 'arr_stop', 'dep_time', 'arr_time', 'trip_id', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'])\n",
    "    timetable['connection_id'] = timetable.groupby('trip_id').cumcount() + 1\n",
    "    timetable['connection_id'] = timetable['connection_id'].astype(str) + '_' + timetable['trip_id']\n",
    "    timetable = timetable.sort_values(by='arr_time')\n",
    "\n",
    "    return timetable\n",
    "\n",
    "# Construct the dataframe (see function description)\n",
    "timetable = construct_schedule_dataframe(df_stops)\n",
    "timetable.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dce88b-2d83-492d-9a33-ebc2342f45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_path = \"data/\"\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "timetable.to_csv(data_path + \"timetable.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5f8818-162c-401b-a6bb-211d94067107",
   "metadata": {},
   "source": [
    "## 5. Stop ID Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998e6615-1cf8-4c0e-9305-e83d5ac1cb48",
   "metadata": {},
   "source": [
    "Since stops id inside isdaten table and the one inside the expected stops' tables may be different, we create a table containing the match between them using their coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49bd1d6-a2f2-47a6-b7ff-e085e1931c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64073153-0e01-4412-be35-03bb325b1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('final-project-{0}'.format(getpass.getuser())).getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "conf = sc.getConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e584df-3dc9-4a96-8592-5942edbf2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "isdaten_stops_df = spark.read.csv('/data/sbb/csv/timetables/stops', header=True)\\\n",
    "                .select(['stop_id','stop_lat', 'stop_lon'])\\\n",
    "                .dropDuplicates(['stop_id'])\\\n",
    "                .withColumnRenamed(\"stop_id\", \"stops_stop_id\")\\\n",
    "                .withColumn(\"stop_lat\", col(\"stop_lat\").cast(DoubleType()))\\\n",
    "                .withColumn(\"stop_lon\", col(\"stop_lon\").cast(DoubleType()))\n",
    "isdaten_stops_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a0fac-1840-405d-ad8b-371acff28806",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/\"\n",
    "isdaten_stops_df.toPandas().to_csv(data_path + \"isdaten_stops_full.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2c808-d9ae-4fc1-a40d-7800205db0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "a = pd.read_csv(\"data/stops.csv\")\n",
    "a = a.rename(columns={\"stop_id\":\"stop_id_a\"})\n",
    "b = pd.read_csv(\"data/isdaten_stops_full.csv\")\n",
    "b = b.rename(columns={\"stops_stop_id\":\"stop_id_b\"})\n",
    "\n",
    "# see: https://en.wikipedia.org/wiki/Haversine_formula\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2)**2\n",
    "    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "closest_stops = {}\n",
    "for _, row_a in a.iterrows():\n",
    "    distances = haversine(row_a['stop_lat'], row_a['stop_lon'], b['stop_lat'], b['stop_lon'])\n",
    "    closest_stop_id = b.loc[np.argmin(distances), 'stop_id_b']\n",
    "    closest_stops[row_a['stop_id_a']] = closest_stop_id\n",
    "    \n",
    "df = pd.DataFrame.from_dict(closest_stops, orient='index', columns=['isdaten_stop_id']).reset_index()\n",
    "df.rename(columns={'index': 'journey_stop_id'}, inplace=True)\n",
    "df.to_csv('data/stop_matching.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c689af-6805-439d-8797-6b1816222898",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d656e-8237-48d7-8d99-93b7c04a9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = f\"/user/{username}/stop_matching.csv\"\n",
    "spark_df.write.csv(hdfs_path, mode=\"overwrite\", header=\"True\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
